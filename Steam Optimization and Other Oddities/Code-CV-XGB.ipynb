{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import xgboost as xgb\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import optuna\n",
    "from utils_testing import optuna_logging\n",
    "import pytz\n",
    "UTC = pytz.utc  \n",
    "timeZ_Kl = pytz.timezone('Asia/Kolkata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef38fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\"../data/train_df_interim.pickle\")\n",
    "test_df = pd.read_pickle(\"../data/test_df_interim.pickle\")\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998d9a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop = ['SURV_DTE'\n",
    "        , 'sand_target_avg'\n",
    "        , 'fold'\n",
    "       ]\n",
    "target = 'PCT_DESAT_TO_ORIG'\n",
    "indep = train_df.columns.difference(drop+[target])\n",
    "indep_master = indep.copy() # Taking a copy so it can be used to get the original features\n",
    "indep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a43b6",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e32266",
   "metadata": {},
   "source": [
    "### 5 fold Groupd CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd4b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval_rmse(preds, dtrain):\n",
    "    actual = dtrain.get_label()\n",
    "    preds = np.where(preds>=1,1, preds)\n",
    "    preds = np.where(preds<=0,0, preds)\n",
    "    \n",
    "    fold_rmse = np.sqrt(mean_squared_error(actual, preds))\n",
    "    \n",
    "    return 'xgb_eval_rmse', fold_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58674d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_model(train_df, xgb_params):\n",
    "    \n",
    "    num_rounds = 100000\n",
    "\n",
    "    fold_iterations = []\n",
    "    fold_results = []\n",
    "    xgb_models_fold = {}\n",
    "\n",
    "    print(\"\")\n",
    "    for fold_i in range(0, train_df.fold.max()+1):\n",
    "\n",
    "        train_fold = train_df[train_df.fold!=fold_i].copy()\n",
    "        valid_fold = train_df[train_df.fold==fold_i].copy()\n",
    "\n",
    "        dtrain_local = xgb.DMatrix(data= train_fold[indep] , label=train_fold[target])\n",
    "        dtest_local = xgb.DMatrix(data= valid_fold[indep] , label=valid_fold[target])\n",
    "\n",
    "        eval_set = [(dtrain_local,'train'), (dtest_local,'test')]\n",
    "\n",
    "        np.random.seed(100)\n",
    "        xgb_model_local = xgb.train(xgb_params,\n",
    "                                    dtrain_local,\n",
    "                                    evals = eval_set,\n",
    "                                    num_boost_round = num_rounds,\n",
    "#                                     feval = xgb_eval_rmse\n",
    "#                                     maximize = False,\n",
    "                                    verbose_eval = False,\n",
    "                                    early_stopping_rounds = 50)\n",
    "        xgb_local_prediction = xgb_model_local.predict(dtest_local)\n",
    "\n",
    "        xgb_local_prediction = np.where(xgb_local_prediction<0, 0, xgb_local_prediction)\n",
    "        xgb_local_prediction = np.where(xgb_local_prediction>1, 1, xgb_local_prediction)\n",
    "\n",
    "        fold_rmse = np.sqrt(mean_squared_error(valid_fold[target], xgb_local_prediction))\n",
    "        fold_iteration = xgb_model_local.best_iteration\n",
    "        \n",
    "        fold_iterations.append(fold_iteration)\n",
    "        fold_results.append(np.round(fold_rmse, 5))\n",
    "        xgb_models_fold[fold_i] = xgb_model_local\n",
    "        \n",
    "        print(f\"Current fold: {fold_i}, iteration {fold_iteration}, RMSE {fold_rmse}\")\n",
    "    \n",
    "    return fold_iterations, fold_results, xgb_models_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0295fa15",
   "metadata": {},
   "source": [
    "# XGB optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_model_optuna(trial):\n",
    "    \"\"\"\n",
    "    This function is used to train the model using the parameters obtained from optuna.\n",
    "    \"\"\"\n",
    "    xgb_param = {\n",
    "                'objective' : 'reg:squarederror'\n",
    "                , 'eval_metric': 'rmse'\n",
    "                , 'max_depth' : trial.suggest_int(\"max_depth\", 3, 7)\n",
    "                , 'eta': trial.suggest_float(\"eta\", 0.01, 0.1, log=True)\n",
    "                , 'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.8, 1.0)\n",
    "                , 'subsample': trial.suggest_float(\"subsample\", 0.8, 1.0)\n",
    "                , 'min_child_weight': trial.suggest_float(\"min_child_weight\", 0, 20)\n",
    "            }\n",
    "    \n",
    "\n",
    "    xgb_fold_iterations, xgb_fold_results, xgb_models_fold = train_xgb_model(train_df=train_df, \n",
    "                                                                             xgb_params = xgb_param)\n",
    "    \n",
    "    avg_error = np.mean(xgb_fold_results)\n",
    "    print(\"Avg.Fold results:\", avg_error)\n",
    "\n",
    "    return avg_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878cdce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optuna Hyper-parameter tuning\n",
    "xgb_study = optuna.create_study(direction=\"minimize\")\n",
    "xgb_study.optimize(train_xgb_model_optuna\n",
    "                   , n_trials=50\n",
    "                   , n_jobs=1\n",
    "                   #                , timeout=600\n",
    "                   , show_progress_bar=True\n",
    "                   , gc_after_trial=True\n",
    "              )\n",
    "\n",
    "# Write the best hyer-parameter and the best RMSE to the logging file\n",
    "optuna_logging(model='xgb', study=xgb_study, indep=np.array(indep))\n",
    "\n",
    "print(\"Number of finished trials: \", len(xgb_study.trials))\n",
    "print(\"Best trial:\", xgb_study.best_trial.number)\n",
    "print(\"Best Value: {}\".format(xgb_study.best_trial.value))\n",
    "print(\"Params: \")\n",
    "xgb_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5133a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the hyperparameters and their best RMSE from the logged file\n",
    "filename = f\"../Optuna_logging/xgb_optuna_logging.csv\"\n",
    "temp = pd.read_csv(filename)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_RMSE = temp.best_RMSE.min()\n",
    "xgb_params = eval(temp.best_param[temp.best_RMSE==best_RMSE].values[0])\n",
    "print(f\"The parameter corresponding to the best RMSE {best_RMSE}\")\n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b387e",
   "metadata": {},
   "source": [
    "# Indep combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb68775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indep_combination(indep_all_combo, total_combinations_to_try):\n",
    "    \"\"\"\n",
    "    This function trains the XGB model based on the different combinations of independent \n",
    "    features from the overall features that is available and write it to the file \n",
    "    xgb_best_indep_combo.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    xgb_params = {'objective' : 'reg:squarederror'\n",
    "                  ,'eval_metric': 'rmse'\n",
    "                  ,'max_depth' : 5\n",
    "                  ,'eta' : 0.01\n",
    "                  ,'subsample': 0.9\n",
    "                  ,'colsample_bytree': 0.9\n",
    "                  ,'min_child_weight':20\n",
    "                  ,'gamma': 1\n",
    "        #           ,'tree_method' : 'gpu_hist'\n",
    "                  }\n",
    "\n",
    "    # reading the iterations ran so far\n",
    "    global overall_best\n",
    "    overall_best = pd.read_csv(\"../indep_combo/xgb_best_indep_combo.csv\")\n",
    "#     overall_best['indep'] = overall_best.indep.apply(lambda x : eval(x))\n",
    "\n",
    "    random_index = np.random.choice(len(indep_all_combo), total_combinations_to_try, replace=False)\n",
    "    mean_fold_result = []\n",
    "    best_result={}\n",
    "    indep_df = []\n",
    "\n",
    "    # declare the indep as global so the changes can be reflected in the training function\n",
    "    global indep \n",
    "    \n",
    "    best = 10000\n",
    "    for i, indep_ind in enumerate(random_index):\n",
    "        indep= indep_all_combo[indep_ind]\n",
    "        print(f\"{i}/{total_combinations_to_try}\")\n",
    "\n",
    "        fold_iterations, fold_results, xgb_models_fold = train_xgb_model(train_df=train_df,\n",
    "                                                                         xgb_params = xgb_params)\n",
    "        mean_fold_result.append(np.mean(fold_results))\n",
    "        indep_df.append(indep)\n",
    "        avg_iteration = int(np.mean(fold_iterations))\n",
    "\n",
    "        print(\"Fold iterations:\", fold_iterations)\n",
    "        print(\"Average iteration:\", avg_iteration)\n",
    "        print(\"Fold results:\", fold_results)\n",
    "        print(\"Avg.Fold results:\", mean_fold_result[-1])\n",
    "\n",
    "        # Printing the current best\n",
    "        if mean_fold_result[-1]<best:\n",
    "            best = mean_fold_result[-1]\n",
    "            print(colored(f\"New best {best}\", 'green'))\n",
    "            \n",
    "            # Reading and writing the indep combo\n",
    "            overall_best = pd.read_csv(\"../indep_combo/xgb_best_indep_combo.csv\")\n",
    "            best_indep = pd.DataFrame({'Date':datetime.now(timeZ_Kl).strftime('%d-%m-%Y %H:%M:%S'),\n",
    "                                       'indep': str(indep), \n",
    "                                       'rmse': [best]})\n",
    "            \n",
    "            print(colored(\"writing the indep combos to disk\", 'blue'))\n",
    "            overall_best = overall_best.append(best_indep).drop_duplicates().reset_index(drop=True)\n",
    "            overall_best.to_csv(\"../indep_combo/xgb_best_indep_combo.csv\", index=False)\n",
    "            \n",
    "        else:\n",
    "            print(colored(f\"Best so far {best}\", 'yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9a03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total actual features: {len(indep_master)}\")\n",
    "\n",
    "features_2_use=29\n",
    "comb_features = combinations(indep_master, features_2_use)\n",
    "\n",
    "indep_all_combo=[]\n",
    "for indep_combo in list(comb_features):\n",
    "    indep_all_combo.append(list(indep_combo))\n",
    "    \n",
    "print(f\"Total features to use: {features_2_use}\")\n",
    "print(f\"Total combo possible : {len(indep_all_combo)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23cb838",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indep_combination(indep_all_combo=indep_all_combo, \n",
    "                      total_combinations_to_try=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48701bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the parameters and the independent features with the best metric\n",
    "\n",
    "days_before = 0\n",
    "\n",
    "best_indep = pd.read_csv(\"../indep_combo/xgb_best_indep_combo.csv\")\n",
    "best_indep['Date'] = pd.to_datetime(best_indep.Date).dt.date.astype('str')\n",
    "\n",
    "today_date = (datetime.now()-timedelta(days=days_before)).strftime('%Y-%m-%d')\n",
    "print(today_date)\n",
    "\n",
    "condition1 = (best_indep.Date==today_date)\n",
    "best_indep = best_indep[condition1].reset_index(drop=True)\n",
    "\n",
    "condition2 = (best_indep.rmse == best_indep.rmse.min())\n",
    "indep = eval(best_indep[condition2].indep.values[0])\n",
    "xgb_params = eval(best_indep[condition2].params.values[0])\n",
    "\n",
    "print(f\"Best RMSE : {best_indep.rmse.min()}\")\n",
    "print(\"Best indep size\", len(indep))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de53711",
   "metadata": {},
   "source": [
    "### local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21696086",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb_params = {'objective' : 'reg:squarederror'\n",
    "              ,'eval_metric': 'rmse'\n",
    "              ,'max_depth' : 5\n",
    "              ,'eta' : 0.01\n",
    "              ,'subsample': 0.9\n",
    "              ,'colsample_bytree': 0.9\n",
    "              ,'min_child_weight':20\n",
    "              ,'gamma': 1\n",
    "    #           ,'tree_method' : 'gpu_hist'\n",
    "              }\n",
    "\n",
    "# xgb_params = {'objective' : 'reg:squarederror'\n",
    "#               ,'eval_metric': 'rmse'\n",
    "#               ,'max_depth' : 5\n",
    "#               ,'eta' : 0.01\n",
    "#               ,'subsample': 0.9\n",
    "#               ,'colsample_bytree': 0.9\n",
    "#               ,'min_child_weight':20\n",
    "#               ,'gamma': 1\n",
    "#     #           ,'tree_method' : 'gpu_hist'\n",
    "#               }\n",
    "\n",
    "# xgb_params['objective'] =  'reg:squarederror'\n",
    "# xgb_params['eval_metric'] = 'rmse'\n",
    "\n",
    "fold_iterations, fold_results, xgb_models_fold = train_xgb_model(train_df=train_df, \n",
    "                                                                 xgb_params = xgb_params)\n",
    "\n",
    "avg_iteration = int(np.mean(fold_iterations))\n",
    "print(\"Fold iterations:\", fold_iterations)\n",
    "print(\"Average iteration:\", avg_iteration)\n",
    "print(\"Fold results:\", fold_results)\n",
    "print(\"Avg.Fold results:\", np.mean(fold_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb000446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ind=4\n",
    "xgb_imp = pd.DataFrame({'feature' : xgb_models_fold[ind].get_score().keys(), \n",
    "                        'fea_imp' : xgb_models_fold[ind].get_score().values()}).sort_values(['fea_imp'], ascending=False).reset_index(drop=True)\n",
    "xgb_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3786a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_ensemble(model_list, test):\n",
    "    \"\"\"\n",
    "    This is the Ensemble prediction of the final test data from the fold models\n",
    "    \"\"\"\n",
    "    \n",
    "    dtest_prod = xgb.DMatrix(data= test[indep])\n",
    "    \n",
    "    ens_pred = []\n",
    "    for i in model_list.keys():\n",
    "        print(f\"Prediction for model {i}\")  \n",
    "        \n",
    "        fold_pred = model_list[i].predict(dtest_prod)\n",
    "        fold_pred = np.where(fold_pred<0, 0, fold_pred)\n",
    "        fold_pred = np.where(fold_pred>1, 1, fold_pred)\n",
    "        ens_pred.append(fold_pred)\n",
    "        \n",
    "    ensemble_prediction = np.array(ens_pred).mean(axis=0)\n",
    "           \n",
    "    return ensemble_prediction\n",
    "        \n",
    "# xgb_prod_prediction = fold_ensemble(model_list=xgb_models_fold, test=test_df)\n",
    "# xgb_prod_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69177982",
   "metadata": {},
   "source": [
    "### Prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa94c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_prod = xgb.DMatrix(data= train_df[indep] , label=train_df[target])\n",
    "dtest_prod = xgb.DMatrix(data= test_df[indep])\n",
    "\n",
    "train_prod_iter = avg_iteration #+ int(0.2*avg_iteration)\n",
    "print(f\"Training for {train_prod_iter} iterations\")\n",
    "np.random.seed(100)\n",
    "xgb_model_prod = xgb.train(xgb_params,\n",
    "                           dtrain_prod,\n",
    "#                            evals = eval_set,\n",
    "                           num_boost_round = train_prod_iter,\n",
    "#                             feval = xgb_eval_rmspe,\n",
    "#                             maximize = False,\n",
    "#                            verbose_eval = True,\n",
    "#                            early_stopping_rounds = 50\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda5893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_prod_prediction = xgb_model_prod.predict(dtest_prod)\n",
    "xgb_prod_prediction = np.where(xgb_prod_prediction<0, 0, xgb_prod_prediction)\n",
    "xgb_prod_prediction = np.where(xgb_prod_prediction>1, 1, xgb_prod_prediction)\n",
    "xgb_prod_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32714b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "XGB_submission = pd.DataFrame({'PCT_DESAT_TO_ORIG':xgb_prod_prediction})\n",
    "XGB_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_submission.to_csv(\"../sub/XGB_sub_38.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56846819",
   "metadata": {},
   "source": [
    "# Model Explainability using SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b16f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "explainer = shap.TreeExplainer(xgb_model_prod)\n",
    "shap_values = explainer.shap_values(train_df[indep].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(xgb_model_prod)\n",
    "shap_values = explainer.shap_values(train_df[indep].reset_index(drop=True))\n",
    "\n",
    "i =10\n",
    "shap.force_plot(explainer.expected_value, \n",
    "                shap_values[i], \n",
    "                features=train_df.loc[i, indep], \n",
    "                feature_names=train_df[indep].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f308d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, \n",
    "                  features=train_df[indep].reset_index(drop=True),\n",
    "                  feature_names=train_df[indep].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c4d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
