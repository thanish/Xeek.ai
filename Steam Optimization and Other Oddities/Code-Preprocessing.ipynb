{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40899f3d",
   "metadata": {},
   "source": [
    "# Reading the actual Train and Test csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ea35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "train_df = train_df.drop(['GAS_H', 'RMNG_OIL_H'], axis=1)\n",
    "test_df = pd.read_csv(\"../data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d8cf4",
   "metadata": {},
   "source": [
    "# Create a K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Fold CV for the training data\n",
    "\n",
    "train_df['fold'] = -1\n",
    "\n",
    "np.random.seed(100)\n",
    "n_splits = 5\n",
    "gfold = GroupKFold(n_splits=n_splits)\n",
    "gfold_loop = gfold.split(train_df, groups=train_df.CMPL_FAC_ID)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(gfold_loop):\n",
    "    train_df.loc[valid_index, ['fold']] = i\n",
    "    \n",
    "train_df.to_csv(\"../data/train_data_with_fold.csv\", index=False)\n",
    "\n",
    "print(train_df.fold.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b971373",
   "metadata": {},
   "source": [
    "# Reading the Train csv with fold and the Test csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train_data_with_fold.csv\")\n",
    "test_df = pd.read_csv(\"../data/test_data.csv\")\n",
    "\n",
    "train_df['SURV_DTE'] = pd.to_datetime(train_df['SURV_DTE'])\n",
    "test_df['SURV_DTE'] = pd.to_datetime(test_df['SURV_DTE'])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81541e73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df[['CMPL_FAC_ID', \n",
    "          'SAND', \n",
    "#           'SURV_DTE', \n",
    "          'AVG_ORIG_OIL_SAT', \n",
    "          'ORIG_OIL_H']].drop_duplicates().sort_values(['CMPL_FAC_ID', 'SAND']).tail(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b0392b",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33999882",
   "metadata": {},
   "source": [
    "#### This portion of the code details with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steam_injection_by_dist(df):\n",
    "    \n",
    "    df['steam_injection_by_dist_1'] = (df['FT_DIST_PAT_1']/df['SGMT_CUM_STM_INJ_1']).replace({np.inf:0, -np.inf:0})\n",
    "    df['steam_injection_by_dist_2'] = (df['FT_DIST_PAT_2']/df['SGMT_CUM_STM_INJ_2']).replace({np.inf:0, -np.inf:0})\n",
    "    df['steam_injection_by_dist_3'] = (df['FT_DIST_PAT_3']/df['SGMT_CUM_STM_INJ_3']).replace({np.inf:0, -np.inf:0})\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = get_steam_injection_by_dist(df=train_df)\n",
    "test_df = get_steam_injection_by_dist(df=test_df)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d10d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_injection_per_day(df):\n",
    "    df['total_injection'] = df[['SGMT_CUM_STM_INJ_1', \n",
    "                                'SGMT_CUM_STM_INJ_2', \n",
    "                                'SGMT_CUM_STM_INJ_3']].sum(axis=1)\n",
    "    df['overall_injection'] = df.groupby(['CMPL_FAC_ID', \n",
    "                                          'SURV_DTE']).total_injection.transform('sum')\n",
    "    df['total_injection_INJ_perc_1'] = (df['total_injection']/df['overall_injection']).replace({np.inf:0, -np.inf:0})\n",
    "    df['total_injection_INJ_perc_2'] = (df['total_injection']/df['overall_injection']).replace({np.inf:0, -np.inf:0})\n",
    "    df['total_injection_INJ_perc_3'] = (df['total_injection']/df['overall_injection']).replace({np.inf:0, -np.inf:0})\n",
    "    \n",
    "    df = df.drop(['total_injection'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = get_overall_injection_per_day(df=train_df)\n",
    "test_df = get_overall_injection_per_day(df=test_df)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ace6f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_min_max_dip_difference(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_min_dip_diff', 'fe_max_dip_diff'], axis=1, inplace=True)\n",
    "        test.drop(['fe_min_dip_diff', 'fe_max_dip_diff'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "    \n",
    "    global DIP_min_max_df\n",
    "    DIP_min_max_df = train[['SAND', 'DIP']].append(test[['SAND', 'DIP']])\n",
    "\n",
    "    DIP_min_max_df = DIP_min_max_df.groupby(['SAND']).agg(min_dip = ('DIP', 'min'), \n",
    "                                                          max_dip = ('DIP', 'max')\n",
    "                                                         ).reset_index()\n",
    "    \n",
    "    train = train.merge(DIP_min_max_df, \n",
    "                        left_on='SAND', \n",
    "                        right_on='SAND', \n",
    "                        how='left')\n",
    "    train['fe_min_dip_diff'] = train['DIP'] - train['min_dip']\n",
    "    train['fe_max_dip_diff'] = train['max_dip'] - train['DIP']\n",
    "    \n",
    "    test = test.merge(DIP_min_max_df, \n",
    "                        left_on='SAND', \n",
    "                        right_on='SAND', \n",
    "                        how='left')\n",
    "    test['fe_min_dip_diff'] = test['DIP'] - test['min_dip']\n",
    "    test['fe_max_dip_diff'] = test['max_dip'] - test['DIP']\n",
    "        \n",
    "    train.drop(['min_dip', 'max_dip'], axis=1, inplace=True)\n",
    "    test.drop(['min_dip', 'max_dip'], axis=1, inplace=True)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_min_max_dip_difference(train=train_df, test=test_df)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_target(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_avg_PCT_DESAT_TO_ORIG_lag'], axis=1, inplace=True)\n",
    "        test.drop(['fe_avg_PCT_DESAT_TO_ORIG_lag'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', 'PCT_DESAT_TO_ORIG']].copy()\n",
    "    train_temp = train_temp.sort_values(['SAND', 'SURV_DTE']).reset_index(drop=True)\n",
    "    train_temp_1 = train_temp.groupby(['SAND', \n",
    "                                       'SURV_DTE']).agg(avg_PCT_DESAT_TO_ORIG = ('PCT_DESAT_TO_ORIG', 'mean')).reset_index()\n",
    "    print(f\"Train shape: {train_temp_1.shape}\")\n",
    "    \n",
    "    test_temp = test[['SAND', 'SURV_DTE']].copy()\n",
    "    test_temp = test_temp.sort_values(['SAND', 'SURV_DTE']).drop_duplicates().reset_index(drop=True)\n",
    "    print(f\"Test shape: {test_temp.shape}\")\n",
    "    \n",
    "    train_test_temp_1 = train_temp_1.append(test_temp).drop_duplicates().sort_values(['SAND', 'SURV_DTE']).reset_index(drop=True)\n",
    "    print(f\"Train Test shape: {train_test_temp_1.shape}\")\n",
    "\n",
    "    \n",
    "    global train_test_temp_2\n",
    "    test_dup_condition = ((train_test_temp_1[['SAND', 'SURV_DTE']].duplicated())\n",
    "                          & (train_test_temp_1.avg_PCT_DESAT_TO_ORIG.isnull()))\n",
    "    train_test_temp_2 = train_test_temp_1[~test_dup_condition].reset_index(drop=True)\n",
    "    print(f\"Final df shape {train_test_temp_2.shape}\")\n",
    "    \n",
    "    train_test_temp_2['avg_PCT_DESAT_TO_ORIG'] = train_test_temp_2.groupby(['SAND']).avg_PCT_DESAT_TO_ORIG.fillna(method='ffill')\n",
    "    train_test_temp_2['fe_avg_PCT_DESAT_TO_ORIG_lag'] = train_test_temp_2.groupby(['SAND']).avg_PCT_DESAT_TO_ORIG.shift(1)\n",
    "\n",
    "    train_test_temp_2 = train_test_temp_2.drop(['avg_PCT_DESAT_TO_ORIG'], axis=1)\n",
    "    \n",
    "    train = train.merge(train_test_temp_2, \n",
    "                        left_on = ['SAND', 'SURV_DTE'],\n",
    "                        right_on = ['SAND', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(train_test_temp_2, \n",
    "                      left_on = ['SAND', 'SURV_DTE'], \n",
    "                      right_on = ['SAND', 'SURV_DTE'], \n",
    "                      how='left')    \n",
    "    print(train.shape, test.shape)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_avg_target(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_target(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_avg_sand_PCT_DESAT_TO_ORIG'], axis=1, inplace=True)\n",
    "        test.drop(['fe_avg_sand_PCT_DESAT_TO_ORIG'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "\n",
    "    avg_target_df = train.groupby(['SAND']).agg(fe_avg_sand_PCT_DESAT_TO_ORIG = ('PCT_DESAT_TO_ORIG', 'mean')).reset_index()\n",
    "\n",
    "    train_df = train.merge(avg_target_df, left_on = ['SAND'], right_on = ['SAND'], how='left')\n",
    "    test_df = test.merge(avg_target_df, left_on = ['SAND'], right_on = ['SAND'], how='left')\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# train_df, test_df = get_avg_target(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inj_prod_distance_prod(df):\n",
    "#     df['fe_Lin_dist_inj_prod_factor'] = df.Lin_Dist_Inj_Factor * df.Lin_Dist_Prod_Factor\n",
    "    df['fe_Lin_dist_inj_prod_percent'] = df.Lin_Dist_Inj_Factor / df.Lin_Dist_Prod_Factor\n",
    "    df['fe_Lin_dist_inj_prod_diff'] = df.Lin_Dist_Inj_Factor - df.Lin_Dist_Prod_Factor\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = inj_prod_distance_prod(df=train_df)\n",
    "test_df = inj_prod_distance_prod(df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9b1b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_oil_sat_oil_height(df):\n",
    "    df['fe_oil_volume'] = df['AVG_ORIG_OIL_SAT']*df['ORIG_OIL_H']\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = get_oil_sat_oil_height(df=train_df)\n",
    "test_df = get_oil_sat_oil_height(df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oil_sat_oil_dip(df):\n",
    "    df['fe_oil_sat_DIP'] = df['AVG_ORIG_OIL_SAT']*df['DIP']\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = get_oil_sat_oil_dip(df=train_df)\n",
    "test_df = get_oil_sat_oil_dip(df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf9734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_prod_dip(df):\n",
    "    df['fe_total_prod_dip'] = df['TOTAL_PROD']*df['DIP']\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = get_total_prod_dip(df=train_df)\n",
    "test_df = get_total_prod_dip(df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11179487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_Lin_Dist_prod_Factor_dip(df):\n",
    "    df['fe_Lin_Dist_prod_Factor_dip'] = df['Lin_Dist_Prod_Factor']*df['DIP']\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = get_Lin_Dist_prod_Factor_dip(df=train_df)\n",
    "test_df = get_Lin_Dist_prod_Factor_dip(df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SAND_cummean_DIP(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_SAND_DIP_cum_mean'], axis=1, inplace=True)\n",
    "        test.drop(['fe_SAND_DIP_cum_mean'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'DIP']].copy()\n",
    "    test_temp = test[['SAND', 'SURV_DTE', \n",
    "                      'CMPL_FAC_ID', 'DIP']].copy()\n",
    "    \n",
    "    global avg_cummean_DIP_df\n",
    "    avg_cummean_DIP_df = train_temp.append(test_temp).sort_values(['SAND',\n",
    "                                                                   'SURV_DTE',\n",
    "                                                                   'CMPL_FAC_ID']).reset_index(drop=True)\n",
    "    avg_cummean_DIP_df['fe_SAND_DIP_cum_mean'] = avg_cummean_DIP_df.groupby(['SAND']).DIP.expanding().mean().values\n",
    "#     avg_cummean_DIP_df['fe_SAND_DIP_cum_min'] = avg_cummean_DIP_df.groupby(['SAND']).DIP.expanding().min().values\n",
    "#     avg_cummean_DIP_df['fe_SAND_DIP_cum_max'] = avg_cummean_DIP_df.groupby(['SAND']).DIP.expanding().max().values\n",
    "\n",
    "    avg_cummean_DIP_df = avg_cummean_DIP_df.drop(['DIP'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_cummean_DIP_df, \n",
    "                        left_on=['SAND', 'CMPL_FAC_ID', 'SURV_DTE'], \n",
    "                        right_on=['SAND', 'CMPL_FAC_ID', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_cummean_DIP_df, \n",
    "                      left_on=['SAND', 'CMPL_FAC_ID', 'SURV_DTE'], \n",
    "                      right_on=['SAND', 'CMPL_FAC_ID', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_SAND_cummean_DIP(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff1eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Well_cummean_DIP(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_Well_DIP_cum_mean'], axis=1, inplace=True)\n",
    "        test.drop(['fe_Well_DIP_cum_mean'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'DIP']].copy()\n",
    "    test_temp = test[['SAND', 'SURV_DTE', \n",
    "                      'CMPL_FAC_ID', 'DIP']].copy()\n",
    "    \n",
    "    global avg_cummean_DIP_df\n",
    "    avg_cummean_DIP_df = train_temp.append(test_temp).sort_values(['CMPL_FAC_ID',\n",
    "                                                                   'SURV_DTE',\n",
    "                                                                   'SAND']).reset_index(drop=True)\n",
    "    avg_cummean_DIP_df['fe_Well_DIP_cum_mean'] = avg_cummean_DIP_df.groupby(['CMPL_FAC_ID']).DIP.expanding().mean().values\n",
    "#     avg_cummean_DIP_df['fe_Well_DIP_cum_min'] = avg_cummean_DIP_df.groupby(['CMPL_FAC_ID']).DIP.expanding().min().values\n",
    "#     avg_cummean_DIP_df['fe_Well_DIP_cum_max'] = avg_cummean_DIP_df.groupby(['CMPL_FAC_ID']).DIP.expanding().max().values\n",
    "\n",
    "    avg_cummean_DIP_df = avg_cummean_DIP_df.drop(['DIP'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_cummean_DIP_df, \n",
    "                        left_on=['SAND', 'CMPL_FAC_ID', 'SURV_DTE'], \n",
    "                        right_on=['SAND', 'CMPL_FAC_ID', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_cummean_DIP_df, \n",
    "                      left_on=['SAND', 'CMPL_FAC_ID', 'SURV_DTE'], \n",
    "                      right_on=['SAND', 'CMPL_FAC_ID', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_Well_cummean_DIP(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5aa9ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def well_sand_cummean_AVG_ORIG_OIL_SAT(train, test):\n",
    "    try:\n",
    "        train.drop(['fe_Well_Sand_cummean_AVG_ORIG_OIL_SAT'], axis=1, inplace=True)\n",
    "        test.drop(['fe_Well_Sand_cummean_AVG_ORIG_OIL_SAT'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp =  train[['CMPL_FAC_ID', 'SAND', 'SURV_DTE', 'AVG_ORIG_OIL_SAT']].copy()\n",
    "    test_temp =  test[['CMPL_FAC_ID', 'SAND', 'SURV_DTE', 'AVG_ORIG_OIL_SAT']].copy()\n",
    "    \n",
    "    global well_sand_AVG_ORIG_OIL_SAT_df\n",
    "    \n",
    "    well_sand_AVG_ORIG_OIL_SAT_df = train_temp.append(test_temp).drop_duplicates().reset_index(drop=True)\n",
    "    well_sand_AVG_ORIG_OIL_SAT_df = well_sand_AVG_ORIG_OIL_SAT_df.sort_values(['CMPL_FAC_ID', 'SAND', 'SURV_DTE']).reset_index(drop=True)\n",
    "    \n",
    "    well_sand_AVG_ORIG_OIL_SAT_df['fe_Well_Sand_cummean_AVG_ORIG_OIL_SAT'] = well_sand_AVG_ORIG_OIL_SAT_df.groupby(['CMPL_FAC_ID', 'SAND']).AVG_ORIG_OIL_SAT.expanding().mean().values\n",
    "    \n",
    "    well_sand_AVG_ORIG_OIL_SAT_df = well_sand_AVG_ORIG_OIL_SAT_df.drop(['AVG_ORIG_OIL_SAT'], axis=1)\n",
    "    \n",
    "    train = train.merge(well_sand_AVG_ORIG_OIL_SAT_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                        how='left')\n",
    "    test = test.merge(well_sand_AVG_ORIG_OIL_SAT_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                      how='left')\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "train_df, test_df = well_sand_cummean_AVG_ORIG_OIL_SAT(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cummean_sand_AVG_ORIG_OIL_SAT(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_AVG_ORIG_OIL_SAT_cum_mean'], axis=1, inplace=True)\n",
    "        test.drop(['fe_AVG_ORIG_OIL_SAT_cum_mean'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'AVG_ORIG_OIL_SAT']].copy()\n",
    "    test_temp = test[['SAND', 'SURV_DTE', \n",
    "                      'CMPL_FAC_ID', 'AVG_ORIG_OIL_SAT']].copy()\n",
    "    \n",
    "    avg_orig_oil_sat_df = train_temp.append(test_temp).sort_values(['SAND', \n",
    "                                                                    'SURV_DTE',\n",
    "                                                                    'CMPL_FAC_ID']).reset_index(drop=True)\n",
    "    avg_orig_oil_sat_df['fe_AVG_ORIG_OIL_SAT_cum_mean'] = avg_orig_oil_sat_df.groupby(['SAND']).AVG_ORIG_OIL_SAT.expanding().mean().values\n",
    "\n",
    "    avg_orig_oil_sat_df = avg_orig_oil_sat_df.drop(['AVG_ORIG_OIL_SAT'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_orig_oil_sat_df, \n",
    "                        left_on=['SAND', 'CMPL_FAC_ID', 'SURV_DTE'], \n",
    "                        right_on=['SAND', 'CMPL_FAC_ID', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_orig_oil_sat_df, \n",
    "                      left_on=['SAND', 'CMPL_FAC_ID', 'SURV_DTE'], \n",
    "                      right_on=['SAND', 'CMPL_FAC_ID', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_cummean_sand_AVG_ORIG_OIL_SAT(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['fe_pipe_avg_AVG_ORIG_OIL_SAT'] = train_df.groupby(['CMPL_FAC_ID']).AVG_ORIG_OIL_SAT.transform('mean')\n",
    "# test_df['fe_pipe_avg_AVG_ORIG_OIL_SAT'] = test_df.groupby(['CMPL_FAC_ID']).AVG_ORIG_OIL_SAT.transform('mean')\n",
    "\n",
    "def get_cummean_pipe_AVG_ORIG_OIL_SAT(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_pipe_avg_AVG_ORIG_OIL_SAT_cum_mean',], axis=1, inplace=True)\n",
    "        test.drop(['fe_pipe_avg_AVG_ORIG_OIL_SAT_cum_mean'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'AVG_ORIG_OIL_SAT']].copy()\n",
    "    test_temp = test[['SAND', 'SURV_DTE', \n",
    "                      'CMPL_FAC_ID', 'AVG_ORIG_OIL_SAT']].copy()\n",
    "    \n",
    "    global avg_cummean_AVG_ORIG_OIL_SAT_df\n",
    "    avg_cummean_AVG_ORIG_OIL_SAT_df = train_temp.append(test_temp).sort_values(['CMPL_FAC_ID',\n",
    "                                                                                'SURV_DTE',\n",
    "                                                                                'SAND'\n",
    "                                                                                ]).reset_index(drop=True)\n",
    "    avg_cummean_AVG_ORIG_OIL_SAT_df['fe_pipe_avg_AVG_ORIG_OIL_SAT_cum_mean'] = avg_cummean_AVG_ORIG_OIL_SAT_df.groupby(['CMPL_FAC_ID']).AVG_ORIG_OIL_SAT.expanding().mean().values\n",
    "    \n",
    "    avg_cummean_AVG_ORIG_OIL_SAT_df = avg_cummean_AVG_ORIG_OIL_SAT_df.drop(['AVG_ORIG_OIL_SAT'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_cummean_AVG_ORIG_OIL_SAT_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_cummean_AVG_ORIG_OIL_SAT_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_cummean_pipe_AVG_ORIG_OIL_SAT(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984acd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "def get_cummean_pipe_ORIG_OIL_H(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_pipe_avg_ORIG_OIL_H_cum_mean'], axis=1, inplace=True)\n",
    "        test.drop(['fe_pipe_avg_ORIG_OIL_H_cum_mean'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'ORIG_OIL_H']].copy()\n",
    "    test_temp = test[['SAND', 'SURV_DTE', \n",
    "                      'CMPL_FAC_ID', 'ORIG_OIL_H']].copy()\n",
    "    \n",
    "    global avg_cummean_ORIG_OIL_H_df\n",
    "    avg_cummean_ORIG_OIL_H_df = train_temp.append(test_temp).sort_values(['CMPL_FAC_ID',\n",
    "                                                                          'SURV_DTE',\n",
    "                                                                          'SAND'\n",
    "                                                                         ]).reset_index(drop=True)\n",
    "    avg_cummean_ORIG_OIL_H_df['fe_pipe_avg_ORIG_OIL_H_cum_mean'] = avg_cummean_ORIG_OIL_H_df.groupby(['CMPL_FAC_ID']).ORIG_OIL_H.expanding().mean().values\n",
    "\n",
    "    avg_cummean_ORIG_OIL_H_df = avg_cummean_ORIG_OIL_H_df.drop(['ORIG_OIL_H'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_cummean_ORIG_OIL_H_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_cummean_ORIG_OIL_H_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_cummean_pipe_ORIG_OIL_H(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "def get_cummean_sand_ORIG_OIL_H(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_sand_ORIG_OIL_H_cum_mean'], axis=1, inplace=True)\n",
    "        test.drop(['fe_sand_ORIG_OIL_H_cum_mean'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'ORIG_OIL_H']].copy()\n",
    "    test_temp = test[['SAND', 'SURV_DTE', \n",
    "                      'CMPL_FAC_ID', 'ORIG_OIL_H']].copy()\n",
    "    \n",
    "    global avg_cummean_ORIG_OIL_H_df\n",
    "    avg_cummean_ORIG_OIL_H_df = train_temp.append(test_temp).sort_values(['SAND',\n",
    "                                                                          'SURV_DTE',\n",
    "                                                                          'CMPL_FAC_ID'\n",
    "                                                                         ]).reset_index(drop=True)\n",
    "    avg_cummean_ORIG_OIL_H_df['fe_sand_ORIG_OIL_H_cum_mean'] = avg_cummean_ORIG_OIL_H_df.groupby(['SAND']).ORIG_OIL_H.expanding().mean().values\n",
    "\n",
    "    avg_cummean_ORIG_OIL_H_df = avg_cummean_ORIG_OIL_H_df.drop(['ORIG_OIL_H'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_cummean_ORIG_OIL_H_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_cummean_ORIG_OIL_H_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_cummean_sand_ORIG_OIL_H(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1631c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cummean_pipe_TOTAL_INJ(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_pipe_avg_TOTAL_INJ_cum_mean',\n",
    "                    'fe_pipe_avg_TOTAL_INJ_cum_sum'\n",
    "                    'fe_pipe_avg_TOTAL_INJ_cum_min', \n",
    "                    'fe_pipe_avg_TOTAL_INJ_cum_max'], axis=1, inplace=True)\n",
    "        test.drop(['fe_pipe_avg_TOTAL_INJ_cum_mean',\n",
    "                   'fe_pipe_avg_TOTAL_INJ_cum_sum',\n",
    "                   'fe_pipe_avg_TOTAL_INJ_cum_min', \n",
    "                   'fe_pipe_avg_TOTAL_INJ_cum_max'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'TOTAL_INJ']].copy()\n",
    "    test_temp = test[['SAND', 'SURV_DTE', \n",
    "                      'CMPL_FAC_ID', 'TOTAL_INJ']].copy()\n",
    "    \n",
    "    global avg_cummean_TOTAL_INJ_df\n",
    "    avg_cummean_TOTAL_INJ_df = train_temp.append(test_temp).sort_values(['CMPL_FAC_ID',\n",
    "                                                                                'SURV_DTE',\n",
    "                                                                                'SAND'\n",
    "                                                                                ]).reset_index(drop=True)\n",
    "    avg_cummean_TOTAL_INJ_df['fe_pipe_avg_TOTAL_INJ_cum_mean'] = avg_cummean_TOTAL_INJ_df.groupby(['CMPL_FAC_ID']).TOTAL_INJ.expanding().mean().values\n",
    "    avg_cummean_TOTAL_INJ_df['fe_pipe_avg_TOTAL_INJ_cum_sum'] = avg_cummean_TOTAL_INJ_df.groupby(['CMPL_FAC_ID']).TOTAL_INJ.expanding().sum().values\n",
    "#     avg_cummean_TOTAL_INJ_df['fe_pipe_avg_TOTAL_INJ_cum_min'] = avg_cummean_TOTAL_INJ_df.groupby(['CMPL_FAC_ID']).TOTAL_INJ.expanding().min().values\n",
    "#     avg_cummean_TOTAL_INJ_df['fe_pipe_avg_TOTAL_INJ_cum_max'] = avg_cummean_TOTAL_INJ_df.groupby(['CMPL_FAC_ID']).TOTAL_INJ.expanding().max().values\n",
    "    \n",
    "        # Rolling Mean\n",
    "    avg_cummean_TOTAL_INJ_df['fe_pipe_avg_TOTAL_INJ_rol3_mean'] = avg_cummean_TOTAL_INJ_df.groupby(['CMPL_FAC_ID']).TOTAL_INJ.rolling(3).mean().values\n",
    "    avg_cummean_TOTAL_INJ_df['fe_pipe_avg_TOTAL_INJ_rol3_mean'] = np.where(avg_cummean_TOTAL_INJ_df['fe_pipe_avg_TOTAL_INJ_rol3_mean'].isnull(), \n",
    "                                                                           avg_cummean_TOTAL_INJ_df.TOTAL_INJ,\n",
    "                                                                           avg_cummean_TOTAL_INJ_df['fe_pipe_avg_TOTAL_INJ_rol3_mean'])\n",
    "    \n",
    "    avg_cummean_TOTAL_INJ_df['fe_pipe_avg_TOTAL_INJ_rol5_mean'] = avg_cummean_TOTAL_INJ_df.groupby(['CMPL_FAC_ID']).TOTAL_INJ.rolling(5).mean().values\n",
    "    avg_cummean_TOTAL_INJ_df['fe_pipe_avg_TOTAL_INJ_rol5_mean'] = np.where(avg_cummean_TOTAL_INJ_df['fe_pipe_avg_TOTAL_INJ_rol5_mean'].isnull(), \n",
    "                                                                           avg_cummean_TOTAL_INJ_df.TOTAL_INJ,\n",
    "                                                                           avg_cummean_TOTAL_INJ_df['fe_pipe_avg_TOTAL_INJ_rol5_mean'])\n",
    "\n",
    "    \n",
    "    avg_cummean_TOTAL_INJ_df = avg_cummean_TOTAL_INJ_df.drop(['TOTAL_INJ'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_cummean_TOTAL_INJ_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_cummean_TOTAL_INJ_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_cummean_pipe_TOTAL_INJ(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bfcaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cummean_sand_TOTAL_INJ(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_sand_avg_TOTAL_INJ_cum_mean', \n",
    "                    'fe_sand_avg_TOTAL_INJ_cum_sum', \n",
    "                    'fe_sand_avg_TOTAL_INJ_cum_std',\n",
    "                    'fe_sand_avg_TOTAL_INJ_cum_min', \n",
    "                    'fe_sand_avg_TOTAL_INJ_cum_max'], axis=1, inplace=True)\n",
    "        test.drop(['fe_sand_avg_TOTAL_INJ_cum_mean',\n",
    "                   'fe_sand_avg_TOTAL_INJ_cum_sum', \n",
    "                   'fe_sand_avg_TOTAL_INJ_cum_std',\n",
    "                   'fe_sand_avg_TOTAL_INJ_cum_min', \n",
    "                   'fe_sand_avg_TOTAL_INJ_cum_max'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'TOTAL_INJ']].copy()\n",
    "    test_temp = test[['SAND', 'SURV_DTE', \n",
    "                      'CMPL_FAC_ID', 'TOTAL_INJ']].copy()\n",
    "    \n",
    "    global avg_cummean_TOTAL_INJ_df\n",
    "    avg_cummean_TOTAL_INJ_df = train_temp.append(test_temp).sort_values(['SAND',\n",
    "                                                                         'SURV_DTE',\n",
    "                                                                         'CMPL_FAC_ID'\n",
    "                                                                        ]).reset_index(drop=True)\n",
    "    avg_cummean_TOTAL_INJ_df['fe_sand_avg_TOTAL_INJ_cum_mean'] = avg_cummean_TOTAL_INJ_df.groupby(['SAND']).TOTAL_INJ.expanding().mean().values   \n",
    "    avg_cummean_TOTAL_INJ_df['fe_sand_avg_TOTAL_INJ_cum_sum'] = avg_cummean_TOTAL_INJ_df.groupby(['SAND']).TOTAL_INJ.expanding().sum().values\n",
    "    avg_cummean_TOTAL_INJ_df['fe_sand_avg_TOTAL_INJ_cum_std'] = avg_cummean_TOTAL_INJ_df.groupby(['SAND']).TOTAL_INJ.expanding().std().values\n",
    "    \n",
    "    # Rolling Mean\n",
    "    avg_cummean_TOTAL_INJ_df['fe_sand_avg_TOTAL_INJ_rol3_mean'] = avg_cummean_TOTAL_INJ_df.groupby(['SAND']).TOTAL_INJ.rolling(3).mean().values\n",
    "    avg_cummean_TOTAL_INJ_df['fe_sand_avg_TOTAL_INJ_rol3_mean'] = np.where(avg_cummean_TOTAL_INJ_df['fe_sand_avg_TOTAL_INJ_rol3_mean'].isnull(), \n",
    "                                                                           avg_cummean_TOTAL_INJ_df.TOTAL_INJ,\n",
    "                                                                           avg_cummean_TOTAL_INJ_df['fe_sand_avg_TOTAL_INJ_rol3_mean'])\n",
    "    \n",
    "    avg_cummean_TOTAL_INJ_df['fe_sand_avg_TOTAL_INJ_rol5_mean'] = avg_cummean_TOTAL_INJ_df.groupby(['SAND']).TOTAL_INJ.rolling(5).mean().values\n",
    "    avg_cummean_TOTAL_INJ_df['fe_sand_avg_TOTAL_INJ_rol5_mean'] = np.where(avg_cummean_TOTAL_INJ_df['fe_sand_avg_TOTAL_INJ_rol5_mean'].isnull(), \n",
    "                                                                           avg_cummean_TOTAL_INJ_df.TOTAL_INJ,\n",
    "                                                                           avg_cummean_TOTAL_INJ_df['fe_sand_avg_TOTAL_INJ_rol5_mean'])\n",
    "    \n",
    "    avg_cummean_TOTAL_INJ_df = avg_cummean_TOTAL_INJ_df.drop(['TOTAL_INJ'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_cummean_TOTAL_INJ_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_cummean_TOTAL_INJ_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_cummean_sand_TOTAL_INJ(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171babb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cummean_pipe_TOTAL_GNTL_INJ(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_pipe_avg_TOTAL_GNTL_INJ_cum_mean'], axis=1, inplace=True)\n",
    "        test.drop(['fe_pipe_avg_TOTAL_GNTL_INJ_cum_mean'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'TOTAL_GNTL_INJ']].copy()\n",
    "    test_temp = test[['SAND', 'SURV_DTE', \n",
    "                      'CMPL_FAC_ID', 'TOTAL_GNTL_INJ']].copy()\n",
    "    \n",
    "    global avg_cummean_TOTAL_GNTL_INJ_df\n",
    "    avg_cummean_TOTAL_GNTL_INJ_df = train_temp.append(test_temp).sort_values(['CMPL_FAC_ID',\n",
    "                                                                                'SURV_DTE',\n",
    "                                                                                'SAND'\n",
    "                                                                                ]).reset_index(drop=True)\n",
    "    avg_cummean_TOTAL_GNTL_INJ_df['fe_pipe_avg_TOTAL_GNTL_INJ_cum_mean'] = avg_cummean_TOTAL_GNTL_INJ_df.groupby(['CMPL_FAC_ID']).TOTAL_GNTL_INJ.expanding().mean().values\n",
    "    \n",
    "    # Rolling Mean\n",
    "    avg_cummean_TOTAL_GNTL_INJ_df['fe_pipe_avg_TOTAL_GNTL_INJ_rol3_mean'] = avg_cummean_TOTAL_GNTL_INJ_df.groupby(['SAND']).TOTAL_GNTL_INJ.rolling(3).mean().values\n",
    "    avg_cummean_TOTAL_GNTL_INJ_df['fe_pipe_avg_TOTAL_GNTL_INJ_rol3_mean'] = np.where(avg_cummean_TOTAL_GNTL_INJ_df['fe_pipe_avg_TOTAL_GNTL_INJ_rol3_mean'].isnull(), \n",
    "                                                                                     avg_cummean_TOTAL_GNTL_INJ_df.TOTAL_GNTL_INJ,\n",
    "                                                                                     avg_cummean_TOTAL_GNTL_INJ_df['fe_pipe_avg_TOTAL_GNTL_INJ_rol3_mean'])\n",
    "    \n",
    "    avg_cummean_TOTAL_GNTL_INJ_df['fe_pipe_avg_TOTAL_GNTL_INJ_rol5_mean'] = avg_cummean_TOTAL_GNTL_INJ_df.groupby(['SAND']).TOTAL_GNTL_INJ.rolling(5).mean().values\n",
    "    avg_cummean_TOTAL_GNTL_INJ_df['fe_pipe_avg_TOTAL_GNTL_INJ_rol5_mean'] = np.where(avg_cummean_TOTAL_GNTL_INJ_df['fe_pipe_avg_TOTAL_GNTL_INJ_rol5_mean'].isnull(), \n",
    "                                                                                     avg_cummean_TOTAL_GNTL_INJ_df.TOTAL_GNTL_INJ,\n",
    "                                                                                     avg_cummean_TOTAL_GNTL_INJ_df['fe_pipe_avg_TOTAL_GNTL_INJ_rol5_mean'])\n",
    "\n",
    "    \n",
    "    avg_cummean_TOTAL_GNTL_INJ_df = avg_cummean_TOTAL_GNTL_INJ_df.drop(['TOTAL_GNTL_INJ'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_cummean_TOTAL_GNTL_INJ_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_cummean_TOTAL_GNTL_INJ_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_cummean_pipe_TOTAL_GNTL_INJ(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08edc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cummean_sand_TOTAL_GNTL_INJ(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_sand_avg_TOTAL_GNTL_INJ_cum_mean',\n",
    "                    'fe_sand_avg_TOTAL_GNTL_INJ_cum_sum'], axis=1, inplace=True)\n",
    "        test.drop(['fe_sand_avg_TOTAL_GNTL_INJ_cum_mean',\n",
    "                   'fe_sand_avg_TOTAL_GNTL_INJ_cumsum'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'TOTAL_GNTL_INJ']].copy()\n",
    "    test_temp = test[['SAND', 'SURV_DTE', \n",
    "                      'CMPL_FAC_ID', 'TOTAL_GNTL_INJ']].copy()\n",
    "    \n",
    "    global avg_cummean_TOTAL_GNTL_INJ_df\n",
    "    avg_cummean_TOTAL_GNTL_INJ_df = train_temp.append(test_temp).sort_values(['SAND',\n",
    "                                                                                'SURV_DTE',\n",
    "                                                                                'CMPL_FAC_ID'\n",
    "                                                                                ]).reset_index(drop=True)\n",
    "    avg_cummean_TOTAL_GNTL_INJ_df['fe_sand_avg_TOTAL_GNTL_INJ_cum_mean'] = avg_cummean_TOTAL_GNTL_INJ_df.groupby(['SAND']).TOTAL_GNTL_INJ.expanding().mean().values\n",
    "    avg_cummean_TOTAL_GNTL_INJ_df['fe_sand_avg_TOTAL_GNTL_INJ_cum_sum'] = avg_cummean_TOTAL_GNTL_INJ_df.groupby(['SAND']).TOTAL_GNTL_INJ.expanding().sum().values\n",
    "    \n",
    "    avg_cummean_TOTAL_GNTL_INJ_df = avg_cummean_TOTAL_GNTL_INJ_df.drop(['TOTAL_GNTL_INJ'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_cummean_TOTAL_GNTL_INJ_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_cummean_TOTAL_GNTL_INJ_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_cummean_sand_TOTAL_GNTL_INJ(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabce40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cummean_PCT_DESAT_TO_ORIG(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_PCT_DESAT_TO_ORIG_cum_mean'], axis=1, inplace=True)\n",
    "        test.drop(['fe_PCT_DESAT_TO_ORIG_cum_mean'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'PCT_DESAT_TO_ORIG']].copy()\n",
    "    \n",
    "    global avg_PCT_DESAT_TO_ORIG_df\n",
    "    avg_PCT_DESAT_TO_ORIG_df = train_temp.sort_values(['SAND',                                                                     \n",
    "                                                       'SURV_DTE',\n",
    "                                                       'CMPL_FAC_ID']).reset_index(drop=True)\n",
    "    avg_PCT_DESAT_TO_ORIG_df['fe_PCT_DESAT_TO_ORIG_cum_mean'] = avg_PCT_DESAT_TO_ORIG_df.groupby(['SAND']).PCT_DESAT_TO_ORIG.expanding().mean().values\n",
    "\n",
    "    avg_PCT_DESAT_TO_ORIG_df = avg_PCT_DESAT_TO_ORIG_df.drop(['PCT_DESAT_TO_ORIG'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_PCT_DESAT_TO_ORIG_df, \n",
    "                        left_on=['SAND'], \n",
    "                        right_on=['SAND'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_PCT_DESAT_TO_ORIG_df, \n",
    "                      left_on=['SAND'], \n",
    "                      right_on=['SAND'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_cummean_AVG_ORIG_OIL_SAT(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cummean_pipe_TOTAL_PROD(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_pipe_TOTAL_PROD_cum_mean'], axis=1, inplace=True)\n",
    "        test.drop(['fe_pipe_TOTAL_PROD_cum_mean'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'TOTAL_PROD']].copy()\n",
    "    test_temp = test[['SAND', 'SURV_DTE', \n",
    "                      'CMPL_FAC_ID', 'TOTAL_PROD']].copy()\n",
    "    \n",
    "    global avg_cummean_TOTAL_PROD_df\n",
    "    avg_cummean_TOTAL_PROD_df = train_temp.append(test_temp).sort_values(['CMPL_FAC_ID',\n",
    "                                                                          'SURV_DTE',\n",
    "                                                                          'SAND'\n",
    "                                                                         ]).reset_index(drop=True)\n",
    "    avg_cummean_TOTAL_PROD_df['fe_pipe_TOTAL_PROD_cum_mean'] = avg_cummean_TOTAL_PROD_df.groupby(['CMPL_FAC_ID']).TOTAL_PROD.expanding().mean().values\n",
    "\n",
    "    \n",
    "    # Rolling Mean\n",
    "    avg_cummean_TOTAL_PROD_df['fe_pipe_avg_TOTAL_PROD_rol3_mean'] = avg_cummean_TOTAL_PROD_df.groupby(['CMPL_FAC_ID']).TOTAL_PROD.rolling(3).mean().values\n",
    "    avg_cummean_TOTAL_PROD_df['fe_pipe_avg_TOTAL_PROD_rol3_mean'] = np.where(avg_cummean_TOTAL_PROD_df['fe_pipe_avg_TOTAL_PROD_rol3_mean'].isnull(), \n",
    "                                                                             avg_cummean_TOTAL_PROD_df.TOTAL_PROD,\n",
    "                                                                             avg_cummean_TOTAL_PROD_df['fe_pipe_avg_TOTAL_PROD_rol3_mean'])\n",
    "\n",
    "    avg_cummean_TOTAL_PROD_df['fe_pipe_avg_TOTAL_PROD_rol5_mean'] = avg_cummean_TOTAL_PROD_df.groupby(['CMPL_FAC_ID']).TOTAL_PROD.rolling(5).mean().values\n",
    "    avg_cummean_TOTAL_PROD_df['fe_pipe_avg_TOTAL_PROD_rol5_mean'] = np.where(avg_cummean_TOTAL_PROD_df['fe_pipe_avg_TOTAL_PROD_rol3_mean'].isnull(), \n",
    "                                                                             avg_cummean_TOTAL_PROD_df.TOTAL_PROD,\n",
    "                                                                             avg_cummean_TOTAL_PROD_df['fe_pipe_avg_TOTAL_PROD_rol3_mean'])\n",
    "    \n",
    "    avg_cummean_TOTAL_PROD_df = avg_cummean_TOTAL_PROD_df.drop(['TOTAL_PROD'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_cummean_TOTAL_PROD_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_cummean_TOTAL_PROD_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_cummean_pipe_TOTAL_PROD(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e78dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cummean_sand_TOTAL_PROD(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_sand_TOTAL_PROD_cum_mean'], axis=1, inplace=True)\n",
    "        test.drop(['fe_sand_TOTAL_PROD_cum_mean'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'TOTAL_PROD']].copy()\n",
    "    test_temp = test[['SAND', 'SURV_DTE', \n",
    "                      'CMPL_FAC_ID', 'TOTAL_PROD']].copy()\n",
    "    \n",
    "    global avg_cummean_TOTAL_PROD_df\n",
    "    avg_cummean_TOTAL_PROD_df = train_temp.append(test_temp).sort_values(['SAND',\n",
    "                                                                          'SURV_DTE',\n",
    "                                                                          'CMPL_FAC_ID'\n",
    "                                                                         ]).reset_index(drop=True)\n",
    "    avg_cummean_TOTAL_PROD_df['fe_sand_TOTAL_PROD_cum_mean'] = avg_cummean_TOTAL_PROD_df.groupby(['SAND']).TOTAL_PROD.expanding().mean().values\n",
    "\n",
    "    # Rolling Mean\n",
    "    avg_cummean_TOTAL_PROD_df['fe_sand_avg_TOTAL_PROD_rol3_mean'] = avg_cummean_TOTAL_PROD_df.groupby(['SAND']).TOTAL_PROD.rolling(3).mean().values\n",
    "    avg_cummean_TOTAL_PROD_df['fe_sand_avg_TOTAL_PROD_rol3_mean'] = np.where(avg_cummean_TOTAL_PROD_df['fe_sand_avg_TOTAL_PROD_rol3_mean'].isnull(), \n",
    "                                                                             avg_cummean_TOTAL_PROD_df.TOTAL_PROD,\n",
    "                                                                             avg_cummean_TOTAL_PROD_df['fe_sand_avg_TOTAL_PROD_rol3_mean'])\n",
    "    \n",
    "    avg_cummean_TOTAL_PROD_df['fe_sand_avg_TOTAL_PROD_rol5_mean'] = avg_cummean_TOTAL_PROD_df.groupby(['SAND']).TOTAL_PROD.rolling(5).mean().values\n",
    "    avg_cummean_TOTAL_PROD_df['fe_sand_avg_TOTAL_PROD_rol5_mean'] = np.where(avg_cummean_TOTAL_PROD_df['fe_sand_avg_TOTAL_PROD_rol3_mean'].isnull(), \n",
    "                                                                             avg_cummean_TOTAL_PROD_df.TOTAL_PROD,\n",
    "                                                                             avg_cummean_TOTAL_PROD_df['fe_sand_avg_TOTAL_PROD_rol3_mean'])\n",
    "\n",
    "    \n",
    "    avg_cummean_TOTAL_PROD_df = avg_cummean_TOTAL_PROD_df.drop(['TOTAL_PROD'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_cummean_TOTAL_PROD_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_cummean_TOTAL_PROD_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_cummean_sand_TOTAL_PROD(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f40490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_sand_cummean_TOTAL_PROD(train, test):\n",
    "    try:\n",
    "        train.drop(['fe_Well_Sand_cummean_TOTAL_PROD'], axis=1, inplace=True)\n",
    "        test.drop(['fe_Well_Sand_cummean_TOTAL_PROD'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp =  train[['CMPL_FAC_ID', 'SAND', 'SURV_DTE', 'TOTAL_PROD']].copy()\n",
    "    test_temp =  test[['CMPL_FAC_ID', 'SAND', 'SURV_DTE', 'TOTAL_PROD']].copy()\n",
    "    \n",
    "    global well_sand_TOTAL_PROD_df\n",
    "    \n",
    "    well_sand_TOTAL_PROD_df = train_temp.append(test_temp).drop_duplicates().reset_index(drop=True)\n",
    "    well_sand_TOTAL_PROD_df = well_sand_TOTAL_PROD_df.sort_values(['CMPL_FAC_ID', 'SAND', 'SURV_DTE']).reset_index(drop=True)\n",
    "    \n",
    "    well_sand_TOTAL_PROD_df['fe_Well_Sand_cummean_TOTAL_PROD'] = well_sand_TOTAL_PROD_df.groupby(['CMPL_FAC_ID', 'SAND']).TOTAL_PROD.expanding().mean().values\n",
    "    \n",
    "    well_sand_TOTAL_PROD_df = well_sand_TOTAL_PROD_df.drop(['TOTAL_PROD'], axis=1)\n",
    "    \n",
    "    train = train.merge(well_sand_TOTAL_PROD_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                        how='left')\n",
    "    test = test.merge(well_sand_TOTAL_PROD_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                      how='left')\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "train_df, test_df = well_sand_cummean_TOTAL_PROD(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d707f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_wells(df):\n",
    "    df['fe_total_wells'] = ((~df.FT_DIST_PAT_1.isnull()).astype('int') + \n",
    "                            (~df.FT_DIST_PAT_2.isnull()).astype('int') + \n",
    "                            (~df.FT_DIST_PAT_3.isnull()).astype('int')\n",
    "                        )\n",
    "    return df\n",
    "\n",
    "train_df = total_wells(df=train_df)\n",
    "test_df = total_wells(df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1176ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_well_distance(df):\n",
    "    df['fe_avg_well_distance']= df[['FT_DIST_PAT_1', 'FT_DIST_PAT_2', 'FT_DIST_PAT_3']].mean(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = avg_well_distance(df=train_df)\n",
    "test_df = avg_well_distance(df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd731b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_and_avg_well_injection(df):\n",
    "    \n",
    "#     df['fe_sum_well_injection']= train_df[['SGMT_CUM_STM_INJ_1', 'SGMT_CUM_STM_INJ_2', 'SGMT_CUM_STM_INJ_3']].sum(axis=1)\n",
    "    overall = train_df[['SGMT_CUM_STM_INJ_1', 'SGMT_CUM_STM_INJ_2', 'SGMT_CUM_STM_INJ_3']].sum(axis=1)\n",
    "    df['fe_total_injected_percentage_1'] = (df['SGMT_CUM_STM_INJ_1']/overall).replace({np.inf:0})\n",
    "    df['fe_total_injected_percentage_2'] = (df['SGMT_CUM_STM_INJ_2']/overall).replace({np.inf:0})\n",
    "    df['fe_total_injected_percentage_3'] = (df['SGMT_CUM_STM_INJ_3']/overall).replace({np.inf:0})\n",
    "    \n",
    "    df['fe_avg_well_injection']= overall/3\n",
    "    \n",
    "#     df.drop(['fe_sum_well_injection'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "train_df = total_and_avg_well_injection(df=train_df)\n",
    "test_df = total_and_avg_well_injection(df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bda36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_wells_per_sand(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_unique_well_count'], axis=1, inplace=True)\n",
    "        test.drop(['fe_unique_well_count'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp =  train[['SAND', 'CMPL_FAC_ID', 'SURV_DTE']].copy()\n",
    "    test_temp =  test[['SAND', 'CMPL_FAC_ID', 'SURV_DTE']].copy()\n",
    "    \n",
    "    global uniq_well_df\n",
    "    uniq_well_df = train_temp.append(test_temp).drop_duplicates().reset_index(drop=True)\n",
    "    uniq_well_df = uniq_well_df.groupby(['SAND', 'CMPL_FAC_ID']).agg(min_date=('SURV_DTE', 'min')).reset_index()\n",
    "    uniq_well_df = uniq_well_df.sort_values(['SAND', 'min_date', 'CMPL_FAC_ID']).reset_index(drop=True)\n",
    "    uniq_well_df['fe_unique_well_count'] = uniq_well_df.groupby(['SAND']).CMPL_FAC_ID.cumcount()\n",
    "        \n",
    "    uniq_well_df = uniq_well_df.drop(['min_date'], axis=1)\n",
    "    \n",
    "    train = train.merge(uniq_well_df, \n",
    "                        left_on=['SAND', 'CMPL_FAC_ID'], \n",
    "                        right_on=['SAND', 'CMPL_FAC_ID'], \n",
    "                        how='left')\n",
    "    test = test.merge(uniq_well_df, \n",
    "                      left_on=['SAND', 'CMPL_FAC_ID'], \n",
    "                      right_on=['SAND', 'CMPL_FAC_ID'], \n",
    "                      how='left')\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "train_df, test_df = total_wells_per_sand(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c20912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_sand_per_wells(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_unique_sand_per_well_count'], axis=1, inplace=True)\n",
    "        test.drop(['fe_unique_sand_per_well_count'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp =  train[['SAND', 'CMPL_FAC_ID', 'SURV_DTE']].copy()\n",
    "    test_temp =  test[['SAND', 'CMPL_FAC_ID', 'SURV_DTE']].copy()\n",
    "    \n",
    "    global uniq_sand_df\n",
    "    uniq_sand_df = train_temp.append(test_temp).drop_duplicates().reset_index(drop=True)\n",
    "    uniq_sand_df = uniq_sand_df.groupby(['CMPL_FAC_ID', 'SAND']).agg(min_date=('SURV_DTE', 'min')).reset_index()\n",
    "    uniq_sand_df = uniq_sand_df.sort_values(['CMPL_FAC_ID', 'min_date', 'SAND']).reset_index(drop=True)\n",
    "    uniq_sand_df['fe_unique_sand_per_well_count'] = uniq_sand_df.groupby(['CMPL_FAC_ID']).SAND.cumcount()\n",
    "        \n",
    "    uniq_sand_df = uniq_sand_df.drop(['min_date'], axis=1)\n",
    "    \n",
    "    train = train.merge(uniq_sand_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND'], \n",
    "                        right_on=['CMPL_FAC_ID', 'SAND'], \n",
    "                        how='left')\n",
    "    test = test.merge(uniq_sand_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND'], \n",
    "                      right_on=['CMPL_FAC_ID', 'SAND'], \n",
    "                      how='left')\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "train_df, test_df = total_sand_per_wells(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec0cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sand_age(train, test):\n",
    "    \n",
    "    train_temp =  train[['SAND', 'SURV_DTE']].copy()\n",
    "    test_temp =  test[['SAND', 'SURV_DTE']].copy()\n",
    "    \n",
    "    global sand_min_age_df\n",
    "    SAND_age_df = train_temp.append(test_temp).drop_duplicates().reset_index(drop=True)\n",
    "    sand_min_age_df = SAND_age_df.groupby(['SAND']).agg(sand_min_age = ('SURV_DTE', 'min')).reset_index()\n",
    "    \n",
    "    train = train.merge(sand_min_age_df, \n",
    "                        left_on=['SAND'], \n",
    "                        right_on=['SAND'], \n",
    "                        how='left')\n",
    "    test = test.merge(sand_min_age_df, \n",
    "                      left_on=['SAND'], \n",
    "                      right_on=['SAND'], \n",
    "                      how='left')    \n",
    "    \n",
    "    train['fe_SAND_AGE'] = (train.SURV_DTE - train.sand_min_age).dt.days\n",
    "    test['fe_SAND_AGE'] = (test.SURV_DTE - test.sand_min_age).dt.days\n",
    "    \n",
    "    train.drop(['sand_min_age'], axis=1, inplace=True)\n",
    "    test.drop(['sand_min_age'], axis=1, inplace=True)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = sand_age(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_age(df):\n",
    "    \n",
    "    well_min_age_df = df.groupby(['CMPL_FAC_ID']).agg(well_min_age = ('SURV_DTE', 'min')).reset_index()\n",
    "    df = df.merge(well_min_age_df, left_on=['CMPL_FAC_ID'], right_on=['CMPL_FAC_ID'], how='left')\n",
    "    df['fe_WELL_AGE'] = (df.SURV_DTE - df.well_min_age).dt.days\n",
    "    df.drop(['well_min_age'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = well_age(df=train_df)\n",
    "test_df = well_age(df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_sand_age(df):\n",
    "    \n",
    "    well_sand_min_age_df = df.groupby(['SAND', 'CMPL_FAC_ID']).agg(well_sand_min_age = ('SURV_DTE', 'min')).reset_index()\n",
    "    df = df.merge(well_sand_min_age_df, \n",
    "                  left_on=['SAND', 'CMPL_FAC_ID'], \n",
    "                  right_on=['SAND', 'CMPL_FAC_ID'], \n",
    "                  how='left')\n",
    "    df['fe_WELL_SAND_AGE'] = (df.SURV_DTE - df.well_sand_min_age).dt.days\n",
    "    df.drop(['well_sand_min_age'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = well_sand_age(df=train_df)\n",
    "test_df = well_sand_age(df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268eeccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAND_last_active(train, test):\n",
    "    try:\n",
    "        train.drop(['SAND_last_active'], axis=1, inplace=True)\n",
    "        test.drop(['SAND_last_active'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp =  train[['SAND', 'SURV_DTE']].copy()\n",
    "    test_temp =  test[['SAND', 'SURV_DTE']].copy()\n",
    "    \n",
    "    last_active_df = train_temp.append(test_temp).drop_duplicates().reset_index(drop=True)\n",
    "    last_active_df = last_active_df.sort_values(['SAND', 'SURV_DTE']).reset_index(drop=True)\n",
    "    \n",
    "    last_active_df['SURV_DTE_lag'] = last_active_df.groupby(['SAND']).SURV_DTE.shift(1)\n",
    "    last_active_df['fe_SAND_last_active'] = (last_active_df.SURV_DTE - last_active_df.SURV_DTE_lag).dt.days\n",
    "    last_active_df = last_active_df.drop(['SURV_DTE_lag'], axis=1)\n",
    "#     last_active_df = last_active_df.fillna(0)\n",
    "    \n",
    "    train = train.merge(last_active_df, \n",
    "                        left_on=['SAND', 'SURV_DTE'], \n",
    "                        right_on=['SAND', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(last_active_df, \n",
    "                      left_on=['SAND', 'SURV_DTE'], \n",
    "                      right_on=['SAND', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "train_df, test_df = SAND_last_active(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f6ce99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def WELL_last_active(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['Well_last_active'], axis=1, inplace=True)\n",
    "        test.drop(['Well_last_active'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "    \n",
    "    train_temp =  train[['CMPL_FAC_ID', 'SURV_DTE']].copy()\n",
    "    test_temp =  test[['CMPL_FAC_ID', 'SURV_DTE']].copy()\n",
    "    \n",
    "    last_active_df = train_temp.append(test_temp).drop_duplicates().reset_index(drop=True)\n",
    "    last_active_df = last_active_df.sort_values(['CMPL_FAC_ID', 'SURV_DTE']).reset_index(drop=True)\n",
    "    \n",
    "    last_active_df['SURV_DTE_lag'] = last_active_df.groupby(['CMPL_FAC_ID']).SURV_DTE.shift(1)\n",
    "    last_active_df['fe_Well_last_active'] = (last_active_df.SURV_DTE - last_active_df.SURV_DTE_lag).dt.days\n",
    "    last_active_df = last_active_df.drop(['SURV_DTE_lag'], axis=1)\n",
    "#     last_active_df = last_active_df.fillna(0)\n",
    "    \n",
    "    train = train.merge(last_active_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SURV_DTE'],\n",
    "                        right_on=['CMPL_FAC_ID', 'SURV_DTE'],\n",
    "                        how='left')\n",
    "    test = test.merge(last_active_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SURV_DTE'],\n",
    "                      right_on=['CMPL_FAC_ID', 'SURV_DTE'],\n",
    "                      how='left')\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "train_df, test_df = WELL_last_active(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2343d16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def well_sand_last_active(train, test):\n",
    "    try:\n",
    "        train.drop(['Well_Sand_last_active'], axis=1, inplace=True)\n",
    "        test.drop(['Well_Sand_last_active'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp =  train[['CMPL_FAC_ID', 'SAND', 'SURV_DTE']].copy()\n",
    "    test_temp =  test[['CMPL_FAC_ID', 'SAND', 'SURV_DTE']].copy()\n",
    "    \n",
    "    last_active_df = train_temp.append(test_temp).drop_duplicates().reset_index(drop=True)\n",
    "    last_active_df = last_active_df.sort_values(['CMPL_FAC_ID', 'SAND', 'SURV_DTE']).reset_index(drop=True)\n",
    "    \n",
    "    last_active_df['SURV_DTE_lag'] = last_active_df.groupby(['CMPL_FAC_ID', 'SAND']).SURV_DTE.shift(1)\n",
    "    last_active_df['fe_Well_Sand_last_active'] = (last_active_df.SURV_DTE - last_active_df.SURV_DTE_lag).dt.days\n",
    "    last_active_df = last_active_df.drop(['SURV_DTE_lag'], axis=1)\n",
    "#     last_active_df = last_active_df.fillna(0)\n",
    "    \n",
    "    train = train.merge(last_active_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                        how='left')\n",
    "    test = test.merge(last_active_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                      how='left')\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "train_df, test_df = well_sand_last_active(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25dcb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_sand_usage(train, test):\n",
    "    try:\n",
    "        train.drop(['Well_Sand_usage_count'], axis=1, inplace=True)\n",
    "        test.drop(['Well_Sand_usage_count'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp =  train[['CMPL_FAC_ID', 'SAND', 'SURV_DTE']].copy()\n",
    "    test_temp =  test[['CMPL_FAC_ID', 'SAND', 'SURV_DTE']].copy()\n",
    "    \n",
    "    global well_sand_usage_df\n",
    "    \n",
    "    well_sand_usage_df = train_temp.append(test_temp).drop_duplicates().reset_index(drop=True)\n",
    "    well_sand_usage_df = well_sand_usage_df.sort_values(['CMPL_FAC_ID', 'SAND', 'SURV_DTE']).reset_index(drop=True)\n",
    "    \n",
    "    well_sand_usage_df['fe_Well_Sand_usage_count'] = well_sand_usage_df.groupby(['CMPL_FAC_ID', 'SAND']).CMPL_FAC_ID.cumcount()\n",
    "    \n",
    "    train = train.merge(well_sand_usage_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                        how='left')\n",
    "    test = test.merge(well_sand_usage_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'],\n",
    "                      how='left')\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "train_df, test_df = well_sand_usage(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_usage(train, test):\n",
    "    try:\n",
    "        train.drop(['Well_usage_count'], axis=1, inplace=True)\n",
    "        test.drop(['Well_usage_count'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp =  train[['CMPL_FAC_ID', 'SURV_DTE']].copy()\n",
    "    test_temp =  test[['CMPL_FAC_ID', 'SURV_DTE']].copy()\n",
    "    \n",
    "    global well_usage_df\n",
    "    \n",
    "    well_usage_df = train_temp.append(test_temp).drop_duplicates().reset_index(drop=True)\n",
    "    well_usage_df = well_usage_df.sort_values(['CMPL_FAC_ID', 'SURV_DTE']).reset_index(drop=True)\n",
    "    \n",
    "    well_usage_df['fe_Well_usage_count'] = well_usage_df.groupby(['CMPL_FAC_ID']).SURV_DTE.cumcount()\n",
    "    \n",
    "    train = train.merge(well_usage_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SURV_DTE'],\n",
    "                        right_on=['CMPL_FAC_ID', 'SURV_DTE'],\n",
    "                        how='left')\n",
    "    test = test.merge(well_usage_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SURV_DTE'],\n",
    "                      right_on=['CMPL_FAC_ID', 'SURV_DTE'],\n",
    "                      how='left')\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "train_df, test_df = well_usage(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efbc5fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sand_usage(train, test):\n",
    "    try:\n",
    "        train.drop(['Sand_usage_count'], axis=1, inplace=True)\n",
    "        test.drop(['Sand_usage_count'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp =  train[['SAND', 'SURV_DTE']].copy()\n",
    "    test_temp =  test[['SAND', 'SURV_DTE']].copy()\n",
    "    \n",
    "    global sand_usage_df\n",
    "    \n",
    "    sand_usage_df = train_temp.append(test_temp).drop_duplicates().reset_index(drop=True)\n",
    "    sand_usage_df = sand_usage_df.sort_values(['SAND', 'SURV_DTE']).reset_index(drop=True)\n",
    "    \n",
    "    sand_usage_df['fe_Sand_usage_count'] = sand_usage_df.groupby(['SAND']).SURV_DTE.cumcount()\n",
    "    \n",
    "    train = train.merge(sand_usage_df, \n",
    "                        left_on=['SAND', 'SURV_DTE'],\n",
    "                        right_on=['SAND', 'SURV_DTE'],\n",
    "                        how='left')\n",
    "    test = test.merge(sand_usage_df, \n",
    "                      left_on=['SAND', 'SURV_DTE'],\n",
    "                      right_on=['SAND', 'SURV_DTE'],\n",
    "                      how='left')\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "train_df, test_df = sand_usage(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac85269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cummean_pipe_fe_injection_difference(train, test):\n",
    "    \n",
    "    try:\n",
    "        train.drop(['fe_pipe_fe_injection_difference_cum_mean'], axis=1, inplace=True)\n",
    "        test.drop(['fe_pipe_fe_injection_difference_cum_mean'], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"First time\")\n",
    "    else:\n",
    "        print(\"Repeat so dropped the column\")\n",
    "        \n",
    "    train_temp = train[['SAND', 'SURV_DTE', \n",
    "                        'CMPL_FAC_ID', 'fe_injection_difference']].copy()\n",
    "    test_temp = test[['SAND', 'SURV_DTE', \n",
    "                      'CMPL_FAC_ID', 'fe_injection_difference']].copy()\n",
    "    \n",
    "    global avg_cummean_fe_injection_difference_df\n",
    "    avg_cummean_fe_injection_difference_df = train_temp.append(test_temp).sort_values(['CMPL_FAC_ID',\n",
    "                                                                          'SURV_DTE',\n",
    "                                                                          'SAND'\n",
    "                                                                         ]).reset_index(drop=True)\n",
    "    avg_cummean_fe_injection_difference_df['fe_pipe_fe_injection_difference_cum_mean'] = avg_cummean_fe_injection_difference_df.groupby(['CMPL_FAC_ID']).fe_injection_difference.expanding().mean().values\n",
    "\n",
    "    avg_cummean_fe_injection_difference_df = avg_cummean_fe_injection_difference_df.drop(['fe_injection_difference'], axis=1)\n",
    "    \n",
    "    train = train.merge(avg_cummean_fe_injection_difference_df, \n",
    "                        left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                        how='left')\n",
    "    test = test.merge(avg_cummean_fe_injection_difference_df, \n",
    "                      left_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      right_on=['CMPL_FAC_ID', 'SAND', 'SURV_DTE'], \n",
    "                      how='left')\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = get_cummean_pipe_fe_injection_difference(train=train_df, test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['fe_sand_reservoir_percentage']  = train_df.TOTAL_GNTL_INJ/ train_df.TOTAL_INJ\n",
    "test_df['fe_sand_reservoir_percentage']  = test_df.TOTAL_GNTL_INJ/ test_df.TOTAL_INJ\n",
    "\n",
    "train_df['fe_prod_inj_percentage']  = train_df.TOTAL_PROD/ train_df.TOTAL_INJ\n",
    "test_df['fe_prod_inj_percentage']  = test_df.TOTAL_PROD/ test_df.TOTAL_INJ\n",
    "\n",
    "train_df['fe_injection_difference'] = train_df.TOTAL_INJ - train_df.TOTAL_GNTL_INJ \n",
    "test_df['fe_injection_difference'] = test_df.TOTAL_INJ - test_df.TOTAL_GNTL_INJ \n",
    "\n",
    "train_df['fe_injection_difference_percentage'] = (train_df.fe_injection_difference/train_df.TOTAL_GNTL_INJ).replace({np.inf:0})\n",
    "test_df['fe_injection_difference_percentage'] = (test_df.fe_injection_difference/test_df.TOTAL_GNTL_INJ).replace({np.inf:0})\n",
    "\n",
    "train_df['fe_injection_difference_reservoir_percentage'] = (train_df.fe_injection_difference/train_df.TOTAL_INJ).replace({np.inf:0, -np.inf:0})\n",
    "test_df['fe_injection_difference_reservoir_percentage'] = (test_df.fe_injection_difference/test_df.TOTAL_INJ).replace({np.inf:0, -np.inf:0})\n",
    "\n",
    "train_df['fe_injection_difference_PROD_percentage'] = (train_df.fe_injection_difference/train_df.TOTAL_PROD).replace({np.inf:0})\n",
    "test_df['fe_injection_difference_PROD_percentage'] = (test_df.fe_injection_difference/test_df.TOTAL_PROD).replace({np.inf:0})\n",
    "\n",
    "train_df['fe_injection_difference_oil_volume'] = train_df['fe_injection_difference']*train_df['ORIG_OIL_H']\n",
    "test_df['fe_injection_difference_oil_volume'] = train_df['fe_injection_difference']*train_df['ORIG_OIL_H']\n",
    "\n",
    "train_df['fe_prod_sand_inje_difference'] = train_df.TOTAL_PROD - train_df.TOTAL_GNTL_INJ\n",
    "test_df['fe_prod_sand_inje_difference'] = test_df.TOTAL_PROD - test_df.TOTAL_GNTL_INJ\n",
    "\n",
    "train_df['fe_prod_reservoir_inje_difference'] = train_df.TOTAL_PROD - train_df.TOTAL_INJ\n",
    "test_df['fe_prod_reservoir_inje_difference'] = test_df.TOTAL_PROD - test_df.TOTAL_INJ\n",
    "\n",
    "train_df['fe_prod_reservoir_inje_difference_prod_percent'] = (train_df.fe_prod_reservoir_inje_difference/train_df.TOTAL_PROD).replace({np.inf:0, -np.inf:0})\n",
    "test_df['fe_prod_reservoir_inje_difference_prod_percent'] = (train_df.fe_prod_reservoir_inje_difference/train_df.TOTAL_PROD).replace({np.inf:0, -np.inf:0})\n",
    "\n",
    "train_df['fe_prod_reservoir_inje_difference_TOTAL_INJ_percent'] = train_df.fe_prod_reservoir_inje_difference/train_df.TOTAL_INJ\n",
    "test_df['fe_prod_reservoir_inje_difference_TOTAL_INJ_percent'] = train_df.fe_prod_reservoir_inje_difference/train_df.TOTAL_INJ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078515ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FIllna\n",
    "train_df = train_df.fillna(-999)\n",
    "test_df = test_df.fillna(-999)\n",
    "\n",
    "# mean_cols = test_df.columns.difference(['SAND', 'SURV_DTE'])\n",
    "# train_df[mean_cols] = train_df[mean_cols].apply(lambda x : x.fillna(x.mean(), axis=0))\n",
    "# test_df[mean_cols]  =  test_df[mean_cols].apply(lambda x : x.fillna(x.mean(), axis=0))\n",
    "\n",
    "# train = train_df.sort_values(['SURV_DTE', 'SAND', 'CMPL_FAC_ID']).reset_index(drop=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encode\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df.SAND)\n",
    "train_df['SAND'] = le.transform(train_df.SAND)\n",
    "test_df['SAND'] = le.transform(test_df.SAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7488ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding Neural Network embeddings as features\n",
    "\n",
    "indep=['AVG_ORIG_OIL_SAT', 'DIP', 'FT_DIST_PAT_1', 'FT_DIST_PAT_2',\n",
    "       'FT_DIST_PAT_3', 'Lin_Dist_Inj_Factor', 'Lin_Dist_Prod_Factor',\n",
    "       'ORIG_OIL_H', 'SAND', 'SGMT_CUM_STM_INJ_1', 'SGMT_CUM_STM_INJ_2',\n",
    "       'SGMT_CUM_STM_INJ_3', 'TOTAL_GNTL_INJ', 'TOTAL_INJ', 'TOTAL_PROD',\n",
    "       'fe_AVG_ORIG_OIL_SAT_cum_mean', 'fe_SAND_AGE', 'fe_SAND_DIP_cum_mean',\n",
    "       'fe_SAND_last_active', 'fe_Sand_usage_count', 'fe_WELL_AGE',\n",
    "       'fe_WELL_SAND_AGE', 'fe_Well_DIP_cum_mean', 'fe_Well_Sand_last_active',\n",
    "       'fe_Well_Sand_usage_count', 'fe_Well_last_active',\n",
    "       'fe_Well_usage_count', 'fe_avg_well_distance', 'fe_avg_well_injection',\n",
    "       'fe_injection_difference', 'fe_injection_difference_PROD_percentage',\n",
    "       'fe_injection_difference_percentage', 'fe_oil_sat_DIP', 'fe_oil_volume',\n",
    "       'fe_pipe_avg_AVG_ORIG_OIL_SAT_cum_mean',\n",
    "       'fe_pipe_avg_ORIG_OIL_H_cum_mean', 'fe_pipe_avg_TOTAL_INJ_cum_mean',\n",
    "       'fe_prod_inj_percentage', 'fe_prod_reservoir_inje_difference',\n",
    "       'fe_prod_sand_inje_difference', 'fe_sand_reservoir_percentage',\n",
    "       'fe_total_injected_percentage_1', 'fe_total_injected_percentage_2',\n",
    "       'fe_total_injected_percentage_3', 'fe_total_wells',\n",
    "       'fe_unique_well_count']\n",
    "\n",
    "class Regression_NN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, 128)\n",
    "        self.hidden_layer_1 = nn.Linear(128, 64)\n",
    "        self.hidden_layer_2 = nn.Linear(64, 32)\n",
    "        self.hidden_layer_3 = nn.Linear(32, 64)\n",
    "        self.hidden_layer_4 = nn.Linear(64, 128)\n",
    "        self.output_layer = nn.Linear(128, input_dim)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.input_layer(x)\n",
    "        output = self.relu(output)\n",
    "        output = self.hidden_layer_1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.hidden_layer_2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.hidden_layer_3(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.hidden_layer_4(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.output_layer(output)\n",
    "#         output = self.relu(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "model = Regression_NN(input_dim=len(indep))\n",
    "\n",
    "def get_NN_embeddings(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df)\n",
    "\n",
    "    df = scaler.transform(df)    \n",
    "    \n",
    "    model_path = f\"../output_models/new__regression_nn_fold_0.bin\"\n",
    "    print(f\"{0} : {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    prediction = model(torch.tensor(df, dtype=torch.float))    \n",
    "    pred = prediction.detach().numpy()#.reshape(-1)\n",
    "    \n",
    "    pred = scaler.inverse_transform(pred)\n",
    "    \n",
    "    NN_embeddings = pd.DataFrame(pred, columns = [f\"nn_embed_{i}\" for i in range(len(indep))])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return NN_embeddings\n",
    "\n",
    "train_nn_embed_df = get_NN_embeddings(df=train_df[indep])\n",
    "train_nn_embed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd9f30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e3c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle(\"../data/train_df_interim.pickle\")\n",
    "test_df.to_pickle(\"../data/test_df_interim.pickle\")\n",
    "\n",
    "train_df.to_csv(\"../data/train_df_interim.csv\", index=False)\n",
    "test_df.to_csv(\"../data/test_df_interim.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
